import OpenAI from 'openai';
import fs from 'fs/promises';
import path from 'path';
import xlsx from 'xlsx';
import type { CashFlowAnalysis, OpenAIAnalysisResult } from '../types.js';

// ==================== SMART AI ROUTING ====================
// We use TWO AI clients for optimal performance:
// 1. OpenAI Direct (GPT-4) ‚Üí CSV/XLSX structured data analysis
// 2. OpenRouter (Gemini) ‚Üí Images/PDFs vision processing

// Client 1: OpenAI Direct for structured data (CSV/XLSX)
const openaiDirect = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  timeout: 180000, // 3 minutes timeout for large file analysis
});

// Client 2: OpenRouter with Gemini for vision tasks (images/PDFs)
const openRouter = new OpenAI({
  apiKey: process.env.OPENROUTER_API_KEY,
  baseURL: 'https://openrouter.ai/api/v1',
  timeout: 180000, // 3 minutes timeout for PDF/image processing
  defaultHeaders: {
    'HTTP-Referer': process.env.YOUR_SITE_URL || 'https://aio-simulator.cmgfinancial.ai',
    'X-Title': 'All-In-One Look Back Simulator',
  },
});

// Model configurations
const TEXT_MODEL = 'gpt-4o'; // OpenAI Direct - excellent for structured data, JSON analysis
const VISION_MODEL = 'google/gemini-2.0-flash-001'; // OpenRouter Gemini - superior vision, fast, cost-effective

console.log(`üîß Smart AI Routing Enabled:`);
console.log(`   üìä Text/CSV/XLSX ‚Üí OpenAI Direct (${TEXT_MODEL})`);
console.log(`   üëÅÔ∏è  Images/PDFs ‚Üí OpenRouter (${VISION_MODEL})`);

/**
 * Note: PDFs are now processed NATIVELY using Gemini 2.0 Flash's PDF capabilities.
 * This eliminates the need for client-side PDF-to-image conversion.
 * The analyzePdf function handles direct PDF analysis.
 */

/**
 * Extract data from Excel/CSV file with intelligent compression
 *
 * OPTIMIZATION: Extract only essential fields and limit to recent months
 * to dramatically reduce token usage while maintaining analysis quality
 */
async function extractDataFromSpreadsheet(filePath: string): Promise<string> {
  try {
    const workbook = xlsx.readFile(filePath);
    const sheetName = workbook.SheetNames[0];
    const worksheet = workbook.Sheets[sheetName];
    const jsonData = xlsx.utils.sheet_to_json(worksheet);

    console.log(`Original spreadsheet has ${jsonData.length} rows`);

    // Debug: Log first row to see column names
    if (jsonData.length > 0) {
      const firstRow = jsonData[0] as Record<string, any>;
      console.log(`üìã CSV Column names:`, Object.keys(firstRow));
      console.log(`üìã First row sample:`, firstRow);
    }

    // Helper to convert Excel serial date to ISO date string
    const excelDateToISO = (excelDate: any): string | null => {
      // If it's already a string date, return as-is
      if (typeof excelDate === 'string') return excelDate;

      // If it's a number, it's an Excel serial date
      if (typeof excelDate === 'number') {
        // Excel dates are days since 12/31/1899
        // JavaScript dates are milliseconds since 1/1/1970
        // Difference: 25569 days
        const jsDate = new Date((excelDate - 25569) * 86400 * 1000);

        // Verify it's a valid date
        if (!isNaN(jsDate.getTime())) {
          return jsDate.toISOString().split('T')[0]; // Return YYYY-MM-DD
        }
      }

      return null;
    };

    // Compress data: Extract only essential fields
    const compressedData = jsonData.map((row: any) => {
      // Try common field names for date, description, amount
      const rawDate = row['Posting Date'] || row['Date'] || row['Transaction Date'] || row['date'];
      const date = excelDateToISO(rawDate);
      const description = row['Description'] || row['Merchant'] || row['description'] || row['memo'];
      const amount = row['Amount'] || row['amount'] || row['Debit'] || row['Credit'];
      const type = row['Type'] || row['Transaction Type'] || row['Category'];

      return { date, description, amount, type };
    }).filter(row => row.date && row.amount); // Filter out invalid rows

    console.log(`üìä After field extraction: ${compressedData.length} rows with valid date+amount`);

    // Sample first few dates to verify parsing
    if (compressedData.length > 0) {
      const sample = compressedData.slice(0, 3);
      console.log(`üìÖ Sample dates from CSV:`, sample.map(r => {
        const rawDate = r.date;
        if (!rawDate) return { raw: null, parsed: null, valid: false };
        return {
          raw: rawDate,
          parsed: new Date(rawDate).toISOString(),
          valid: !isNaN(new Date(rawDate).getTime())
        };
      }));
    }

    // Return ALL valid transactions - no date filtering
    // The AI will analyze the entire history provided by the user
    console.log(`‚úì Using all ${compressedData.length} transactions (no date filtering)`);

    return JSON.stringify(compressedData);
  } catch (error) {
    console.error('Error extracting spreadsheet data:', error);
    throw new Error('Failed to extract data from spreadsheet');
  }
}

/**
 * Analyze image file using vision model via OpenRouter
 * Supports images from various sources including client-side PDF conversions
 */
async function analyzeImage(filePath: string): Promise<string> {
  const TIMEOUT_MS = 60000; // 60 seconds

  try {
    console.log('üîç [1/5] Starting image analysis...');
    console.log(`   üìÅ File: ${filePath}`);

    console.log('üìñ [2/5] Reading image file...');
    const imageBuffer = await fs.readFile(filePath);
    console.log(`   ‚úì File read successfully (${imageBuffer.length} bytes)`);

    console.log('üîÑ [3/5] Converting to base64...');
    const base64Image = imageBuffer.toString('base64');
    const ext = path.extname(filePath).toLowerCase();
    const mimeType = ext === '.png' ? 'image/png' : 'image/jpeg';
    console.log(`   ‚úì Converted to base64 (${base64Image.length} chars, type: ${mimeType})`);

    console.log('üåê [4/5] Calling OpenRouter (Gemini) for vision...');
    console.log(`   üì° Endpoint: ${openRouter.baseURL}`);
    console.log(`   ü§ñ Model: ${VISION_MODEL}`);
    console.log(`   ‚è±Ô∏è  Timeout: ${TIMEOUT_MS / 1000}s`);

    // Create a timeout promise that rejects after TIMEOUT_MS
    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => {
        console.error(`‚è±Ô∏è  ‚ùå TIMEOUT FIRED - No response after ${TIMEOUT_MS / 1000}s`);
        reject(new Error(`Image analysis timed out after ${TIMEOUT_MS / 1000} seconds`));
      }, TIMEOUT_MS);
    });

    console.log('   üöÄ Sending API request NOW...');
    const startTime = Date.now();

    // Create the API call promise using OpenRouter with Gemini
    const apiCallPromise = openRouter.chat.completions.create({
      model: VISION_MODEL,
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: `‚ö†Ô∏è CRITICAL INSTRUCTIONS FOR IMAGE TRANSACTION EXTRACTION ‚ö†Ô∏è

You are extracting transaction data from a bank statement image. This data will be used for financial analysis.

üî¥ MANDATORY REQUIREMENTS:
1. Extract EVERY SINGLE TRANSACTION - no sampling, no skipping
2. Use EXACT dates as shown in the image (do not modify or hallucinate dates)
3. Use EXACT descriptions as shown in the image (do not summarize or shorten)
4. Use EXACT amounts as shown in the image (preserve decimal precision)
5. If the image has 200 transactions, your output must have 200 transactions
6. Count transactions as you extract to ensure completeness

FORMAT REQUIREMENT - Each transaction on a new line:
YYYY-MM-DD | Full Description Text | +/-Amount

EXAMPLES:
2024-10-24 | CMG MORTGAGE INC PAYROLL PPD ID: 9999922657 | +9233.45
2024-10-24 | SO CAL EDISON CO BILL PAYMT 700689315083 | -155.38
2024-10-23 | Payment to Chase card ending in 8435 10/23 | -295.88

CRITICAL RULES:
‚úì EXTRACT EVERY TRANSACTION - Do not skip any rows
‚úì PRESERVE EXACT DATES - Copy dates exactly as shown (MM/DD/YYYY ‚Üí YYYY-MM-DD)
‚úì PRESERVE EXACT DESCRIPTIONS - Do not abbreviate or summarize merchant names
‚úì PRESERVE EXACT AMOUNTS - Keep full precision (e.g., -155.38 not -155)
‚úì DETERMINISTIC - Same image must produce same output every time
‚úì NO FILTERING - Include all transaction types (credits, debits, transfers, fees)

‚ö†Ô∏è VERIFICATION: After extraction, count your transactions and state the total count at the end.

Begin extraction now:`,
            },
            {
              type: 'image_url',
              image_url: {
                url: `data:${mimeType};base64,${base64Image}`,
              },
            },
          ],
        },
      ],
      max_tokens: 16000, // Increased to handle large statements with many transactions
      temperature: 0, // Deterministic output for consistent results
      top_p: 1, // Disable nucleus sampling for maximum consistency
      seed: 42, // Fixed seed for reproducibility (may not be supported by all models)
    }, {
      timeout: TIMEOUT_MS, // SDK timeout as backup
    });

    console.log('   ‚è≥ Waiting for response...');
    // Race the API call against the timeout
    const response = await Promise.race([apiCallPromise, timeoutPromise]);

    const elapsedTime = Date.now() - startTime;
    const extractedContent = response.choices[0]?.message?.content || '';
    console.log(`‚úÖ [5/5] API response received in ${(elapsedTime / 1000).toFixed(2)}s`);
    console.log(`   üìù Response length: ${extractedContent.length} chars`);

    // Count transactions in extracted data for verification
    const transactionLines = extractedContent.split('\n').filter(line => line.includes('|')).length;
    console.log(`   üìä Extracted transactions: ${transactionLines} lines`);

    return extractedContent;
  } catch (error) {
    console.error('‚ùå Error analyzing image:', error);

    // Provide more specific error messages
    if (error instanceof Error) {
      if (error.message.includes('timeout') || error.message.includes('timed out')) {
        throw new Error(`Image analysis timed out after ${TIMEOUT_MS / 1000} seconds. Please try again or use a smaller image.`);
      }
      throw new Error(`Failed to analyze image: ${error.message}`);
    }
    throw new Error('Failed to analyze image with vision model');
  }
}

/**
 * Analyze PDF file using vision model via OpenAI
 * TESTING: Native PDF support (no conversion to images)
 */
async function analyzePdf(filePath: string): Promise<string> {
  const TIMEOUT_MS = 120000; // 120 seconds for PDFs (can be multi-page)

  try {
    console.log('üìÑ [1/4] Starting PDF analysis...');
    console.log(`   üìÅ File: ${filePath}`);

    console.log('üìñ [2/4] Reading PDF file...');
    const pdfBuffer = await fs.readFile(filePath);
    console.log(`   ‚úì File read successfully (${pdfBuffer.length} bytes)`);

    console.log('üîÑ [3/4] Converting to base64...');
    const base64Pdf = pdfBuffer.toString('base64');
    console.log(`   ‚úì Converted to base64 (${base64Pdf.length} chars)`);

    console.log('üåê [4/4] Calling OpenRouter (Gemini) with native PDF...');
    console.log(`   üì° Endpoint: ${openRouter.baseURL}`);
    console.log(`   ü§ñ Model: ${VISION_MODEL}`);
    console.log(`   ‚è±Ô∏è  Timeout: ${TIMEOUT_MS / 1000}s`);

    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => {
        console.error(`‚è±Ô∏è  ‚ùå TIMEOUT FIRED - No response after ${TIMEOUT_MS / 1000}s`);
        reject(new Error(`PDF analysis timed out after ${TIMEOUT_MS / 1000} seconds`));
      }, TIMEOUT_MS);
    });

    console.log('   üöÄ Sending API request NOW with native PDF...');
    const startTime = Date.now();

    // Send PDF directly to OpenRouter (Gemini) using proper file format
    const apiCallPromise = openRouter.chat.completions.create({
      model: VISION_MODEL,
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: `‚ö†Ô∏è CRITICAL INSTRUCTIONS FOR PDF TRANSACTION EXTRACTION ‚ö†Ô∏è

You are extracting transaction data from a bank statement PDF. This data will be used for financial analysis.

üî¥ MANDATORY REQUIREMENTS:
1. Extract EVERY SINGLE TRANSACTION - no sampling, no skipping
2. Use EXACT dates as shown in the PDF (do not modify or hallucinate dates)
3. Use EXACT descriptions as shown in the PDF (do not summarize or shorten)
4. Use EXACT amounts as shown in the PDF (preserve decimal precision)
5. If the PDF has 500 transactions, your output must have 500 transactions
6. Count transactions as you extract to ensure completeness

FORMAT REQUIREMENT - Each transaction on a new line:
YYYY-MM-DD | Full Description Text | +/-Amount

EXAMPLES:
2024-10-24 | CMG MORTGAGE INC PAYROLL PPD ID: 9999922657 | +9233.45
2024-10-24 | SO CAL EDISON CO BILL PAYMT 700689315083 | -155.38
2024-10-23 | Payment to Chase card ending in 8435 10/23 | -295.88

CRITICAL RULES:
‚úì EXTRACT EVERY TRANSACTION - Do not skip any rows
‚úì PRESERVE EXACT DATES - Copy dates exactly as shown (MM/DD/YYYY ‚Üí YYYY-MM-DD)
‚úì PRESERVE EXACT DESCRIPTIONS - Do not abbreviate or summarize merchant names
‚úì PRESERVE EXACT AMOUNTS - Keep full precision (e.g., -155.38 not -155)
‚úì DETERMINISTIC - Same PDF must produce same output every time
‚úì NO FILTERING - Include all transaction types (credits, debits, transfers, fees)

‚ö†Ô∏è VERIFICATION: After extraction, count your transactions and state the total count at the end.

Begin extraction now:`,
            },
            {
              type: 'file' as any, // OpenRouter-specific file type
              file: {
                filename: path.basename(filePath),
                file_data: `data:application/pdf;base64,${base64Pdf}`,
              },
            } as any,
          ],
        },
      ],
      max_tokens: 16000, // Increased to handle 500+ transactions
      temperature: 0, // Deterministic output for consistent results
      top_p: 1, // Disable nucleus sampling for maximum consistency
      seed: 42, // Fixed seed for reproducibility (may not be supported by all models)
    } as any, {
      timeout: TIMEOUT_MS,
    });

    console.log('   ‚è≥ Waiting for response...');
    const response = await Promise.race([apiCallPromise, timeoutPromise]);

    const elapsedTime = Date.now() - startTime;
    const extractedContent = response.choices[0]?.message?.content || '';
    console.log(`‚úÖ [4/4] API response received in ${(elapsedTime / 1000).toFixed(2)}s`);
    console.log(`   üìù Response length: ${extractedContent.length} chars`);

    // Count transactions in extracted data for verification
    const transactionLines = extractedContent.split('\n').filter(line => line.includes('|')).length;
    console.log(`   üìä Extracted transactions: ${transactionLines} lines`);

    return extractedContent;
  } catch (error) {
    console.error('‚ùå Error analyzing PDF:', error);

    if (error instanceof Error) {
      if (error.message.includes('timeout') || error.message.includes('timed out')) {
        throw new Error(`PDF analysis timed out after ${TIMEOUT_MS / 1000} seconds. Please try again.`);
      }
      throw new Error(`Failed to analyze PDF: ${error.message}`);
    }
    throw new Error('Failed to analyze PDF with vision model');
  }
}

/**
 * Estimate token count (rough approximation: 1 token ‚âà 4 characters)
 */
function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4);
}

/**
 * Chunk transactions into batches for processing
 * Ensures each chunk stays under token limits
 */
function chunkTransactions(transactions: any[], maxTransactionsPerChunk: number = 250): any[][] {
  const chunks: any[][] = [];

  for (let i = 0; i < transactions.length; i += maxTransactionsPerChunk) {
    const chunk = transactions.slice(i, i + maxTransactionsPerChunk);
    chunks.push(chunk);
  }

  console.log(`üì¶ Split ${transactions.length} transactions into ${chunks.length} chunks of ~${maxTransactionsPerChunk} each`);

  return chunks;
}

/**
 * Process a single chunk of transactions with AI categorization
 */
async function analyzeTransactionChunk(
  chunkData: any[],
  chunkNumber: number,
  totalChunks: number,
  currentHousingPayment: number
): Promise<OpenAIAnalysisResult> {
  const chunkDataStr = JSON.stringify(chunkData);

  console.log(`\nüîÑ [Chunk ${chunkNumber}/${totalChunks}] Processing ${chunkData.length} transactions...`);
  console.log(`   üìä Chunk size: ${chunkDataStr.length} characters (~${estimateTokens(chunkDataStr)} tokens)`);

  const prompt = `You are a financial analysis expert analyzing a BATCH of transactions from bank statements.

‚ö†Ô∏è CRITICAL: This is CHUNK ${chunkNumber} of ${totalChunks}. Analyze ONLY these transactions independently.
‚ö†Ô∏è The data is ALREADY EXTRACTED as JSON. Your job: Categorize and analyze these transactions.
‚ö†Ô∏è PRESERVE ALL TRANSACTIONS: Include EVERY transaction in your output.

Current housing payment (to exclude from expenses): $${currentHousingPayment}

Pre-Parsed Transaction Data for Chunk ${chunkNumber}:
${chunkDataStr}

Your task: CATEGORIZE and ANALYZE these transactions:

CATEGORIZATION RULES:
- "income": Deposits, paychecks, Zelle/transfers IN (positive amounts or credits)
- "expense": Regular recurring expenses like utilities, bills, subscriptions, groceries
- "housing": Rent or mortgage payments (around $${currentHousingPayment})
- "one-time": Irregular purchases, large transfers, one-time payments

FLAG ANOMALIES:
- Luxury items, unusually large purchases, one-time events, irregular deposits
- Provide specific flag reasons when flagged=true

MONTHLY BREAKDOWN:
- Group by month (YYYY-MM) and calculate income, expenses, net cash flow, transaction count

DEPOSIT FREQUENCY DETECTION (CRITICAL FOR AIO LOAN CALCULATION):
Analyze the INCOME transactions to determine the deposit pattern:
- "weekly": Deposits every 7 days (e.g., every Monday)
- "biweekly": Deposits every 14 days (e.g., every other Friday)
- "semi-monthly": Deposits twice per month (e.g., 1st and 15th, or 10th and 25th)
- "monthly": Deposits once per month (e.g., 1st of month or last day)

Look for:
1. Regular income deposits (payroll, salary, etc.)
2. Count days between consecutive deposits
3. If deposits are 6-8 days apart consistently ‚Üí "weekly"
4. If deposits are 13-15 days apart consistently ‚Üí "biweekly"
5. If deposits are 14-16 days apart AND occur ~2x per month ‚Üí "semi-monthly"
6. If deposits are 28-32 days apart consistently ‚Üí "monthly"
7. If pattern is unclear or mixed ‚Üí "monthly" (default)

CRITICAL RULES:
‚úì PRESERVE EVERY TRANSACTION - Output count must match input count (${chunkData.length} transactions)
‚úì USE EXACT DATES/AMOUNTS/DESCRIPTIONS from input
‚úì DETERMINISTIC - Same input = same output
‚úì NO SAMPLING - Include 100% of transactions

Return COMPACT JSON (omit flagReason when flagged=false):
{
  "transactions": [
    {"date": "YYYY-MM-DD", "description": "Description", "amount": 1234.56, "category": "income", "flagged": false, "monthYear": "2024-08"}
  ],
  "monthlyBreakdown": [
    {"month": "2024-08", "income": 5000.00, "expenses": 2500.00, "netCashFlow": 2500.00, "transactionCount": 45}
  ],
  "totalIncome": 5000.00,
  "totalExpenses": 2500.00,
  "netCashFlow": 2500.00,
  "confidence": 0.85,
  "depositFrequency": "biweekly"
}`;

  const CHUNK_TIMEOUT_MS = 90000; // 90 seconds per chunk

  try {
    const response = await openaiDirect.chat.completions.create({
      model: TEXT_MODEL,
      messages: [
        {
          role: 'system',
          content: 'You are a financial analyst specialized in cash flow analysis. Always respond with valid JSON. Be consistent and deterministic.',
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      response_format: { type: 'json_object' },
      temperature: 0,
      top_p: 1,
      seed: 42,
      max_tokens: 16384,
    }, {
      timeout: CHUNK_TIMEOUT_MS,
    });

    const rawContent = response.choices[0]?.message?.content || '{}';
    const result = JSON.parse(rawContent);

    console.log(`‚úÖ [Chunk ${chunkNumber}/${totalChunks}] Completed:`);
    console.log(`   üìä Transactions returned: ${result.transactions?.length || 0}/${chunkData.length}`);
    console.log(`   üí∞ Income: $${result.totalIncome?.toFixed(2) || 0}, Expenses: $${result.totalExpenses?.toFixed(2) || 0}`);

    if (result.transactions?.length !== chunkData.length) {
      console.warn(`‚ö†Ô∏è  [Chunk ${chunkNumber}] Transaction count mismatch! Expected ${chunkData.length}, got ${result.transactions?.length || 0}`);
    }

    return result;
  } catch (error) {
    console.error(`‚ùå [Chunk ${chunkNumber}/${totalChunks}] Failed:`, error);
    throw error;
  }
}

/**
 * Merge results from multiple chunks
 */
function mergeChunkResults(chunkResults: OpenAIAnalysisResult[]): OpenAIAnalysisResult {
  console.log(`\nüîÄ Merging ${chunkResults.length} chunk results...`);

  // Combine all transactions
  const allTransactions = chunkResults.flatMap(chunk => chunk.transactions || []);

  // Merge monthly breakdowns (group by month and sum values)
  const monthlyMap = new Map<string, any>();

  chunkResults.forEach(chunk => {
    (chunk.monthlyBreakdown || []).forEach(monthData => {
      if (monthlyMap.has(monthData.month)) {
        const existing = monthlyMap.get(monthData.month);
        existing.income += monthData.income;
        existing.expenses += monthData.expenses;
        existing.netCashFlow += monthData.netCashFlow;
        existing.transactionCount += monthData.transactionCount;
      } else {
        monthlyMap.set(monthData.month, { ...monthData });
      }
    });
  });

  const monthlyBreakdown = Array.from(monthlyMap.values()).sort((a, b) =>
    a.month.localeCompare(b.month)
  );

  // Calculate merged totals
  const totalIncome = chunkResults.reduce((sum, chunk) => sum + (chunk.totalIncome || 0), 0);
  const totalExpenses = chunkResults.reduce((sum, chunk) => sum + (chunk.totalExpenses || 0), 0);
  const netCashFlow = totalIncome - totalExpenses;

  // Average confidence scores
  const avgConfidence = chunkResults.reduce((sum, chunk) => sum + (chunk.confidence || 0), 0) / chunkResults.length;

  console.log(`‚úÖ Merge complete:`);
  console.log(`   üìä Total transactions: ${allTransactions.length}`);
  console.log(`   üìÖ Months covered: ${monthlyBreakdown.length}`);
  console.log(`   üí∞ Total Income: $${totalIncome.toFixed(2)}`);
  console.log(`   üí∞ Total Expenses: $${totalExpenses.toFixed(2)}`);
  console.log(`   üí∞ Net Cash Flow: $${netCashFlow.toFixed(2)}`);
  console.log(`   üéØ Avg Confidence: ${(avgConfidence * 100).toFixed(1)}%`);

  return {
    transactions: allTransactions,
    monthlyBreakdown,
    totalIncome,
    totalExpenses,
    netCashFlow,
    confidence: avgConfidence,
    depositFrequency: 'monthly',
    monthlyDeposits: totalIncome / (monthlyBreakdown.length || 1),
    monthlyExpenses: totalExpenses / (monthlyBreakdown.length || 1),
    monthlyLeftover: netCashFlow / (monthlyBreakdown.length || 1),
  };
}

/**
 * Split large data into chunks to stay under token limits
 *
 * OPTIMIZATION: Process data in smaller batches to avoid rate limits
 * Each chunk stays under 15K tokens to safely fit within 30K TPM limit
 */
function chunkData(data: string, maxTokensPerChunk: number = 15000): string[] {
  const estimatedTokens = estimateTokens(data);

  if (estimatedTokens <= maxTokensPerChunk) {
    console.log(`Data fits in single chunk (${estimatedTokens} tokens)`);
    return [data];
  }

  console.log(`Data too large (${estimatedTokens} tokens), splitting into chunks...`);

  try {
    // Parse JSON array
    const transactions = JSON.parse(data);

    if (!Array.isArray(transactions)) {
      console.log('Data is not an array, returning as single chunk');
      return [data];
    }

    // Calculate transactions per chunk
    const avgCharsPerTransaction = data.length / transactions.length;
    const avgTokensPerTransaction = avgCharsPerTransaction / 4;
    const transactionsPerChunk = Math.floor(maxTokensPerChunk / avgTokensPerTransaction);

    console.log(`Splitting ${transactions.length} transactions into chunks of ~${transactionsPerChunk} each`);

    const chunks: string[] = [];
    for (let i = 0; i < transactions.length; i += transactionsPerChunk) {
      const chunk = transactions.slice(i, i + transactionsPerChunk);
      chunks.push(JSON.stringify(chunk));
      console.log(`Chunk ${chunks.length}: ${chunk.length} transactions (~${estimateTokens(JSON.stringify(chunk))} tokens)`);
    }

    return chunks;
  } catch (error) {
    console.error('Error chunking data:', error);
    // If parsing fails, return original data as single chunk
    return [data];
  }
}

/**
 * Process a single bank statement file with retry logic
 * Supports native PDF processing via Gemini 2.0 Flash
 */
async function processFile(file: Express.Multer.File, retries = 2): Promise<string> {
  const ext = path.extname(file.originalname).toLowerCase();

  if (ext === '.csv' || ext === '.xlsx' || ext === '.xls') {
    return await extractDataFromSpreadsheet(file.path);
  } else if (ext === '.pdf') {
    // Retry logic for PDF analysis with native PDF support
    let lastError: Error | null = null;
    for (let attempt = 1; attempt <= retries; attempt++) {
      try {
        console.log(`Attempt ${attempt}/${retries} for PDF ${file.originalname}`);
        return await analyzePdf(file.path);
      } catch (error) {
        lastError = error as Error;
        console.error(`Attempt ${attempt}/${retries} failed:`, error instanceof Error ? error.message : error);
        if (attempt < retries) {
          console.log(`Retrying in 2 seconds...`);
          await new Promise(resolve => setTimeout(resolve, 2000));
        }
      }
    }
    throw lastError || new Error('Failed to process PDF after retries');
  } else if (['.jpg', '.jpeg', '.png', '.gif', '.webp'].includes(ext)) {
    // Retry logic for image analysis
    let lastError: Error | null = null;
    for (let attempt = 1; attempt <= retries; attempt++) {
      try {
        console.log(`Attempt ${attempt}/${retries} for ${file.originalname}`);
        return await analyzeImage(file.path);
      } catch (error) {
        lastError = error as Error;
        console.error(`Attempt ${attempt}/${retries} failed:`, error instanceof Error ? error.message : error);
        if (attempt < retries) {
          console.log(`Retrying in 2 seconds...`);
          await new Promise(resolve => setTimeout(resolve, 2000));
        }
      }
    }
    throw lastError || new Error('Failed to process image after retries');
  } else {
    throw new Error(`Unsupported file type: ${ext}`);
  }
}

/**
 * Deduplicate transactions based on date, amount, and description similarity
 * Returns unique transactions and duplicates separately
 */
function deduplicateTransactions(transactions: any[]): {
  uniqueTransactions: any[];
  duplicateTransactions: any[];
} {
  const uniqueTransactions: any[] = [];
  const duplicateTransactions: any[] = [];
  const transactionKeys = new Map<string, any>();

  // Helper to create a transaction key
  const createTransactionKey = (t: any): string => {
    // Normalize description: lowercase, remove extra spaces, remove special chars
    const normalizedDesc = (t.description || '')
      .toLowerCase()
      .replace(/[^a-z0-9\s]/g, '')
      .replace(/\s+/g, ' ')
      .trim()
      .substring(0, 50); // First 50 chars for comparison

    // Normalize amount to 2 decimal places
    const normalizedAmount = Math.abs(parseFloat(t.amount) || 0).toFixed(2);

    // Parse and normalize date
    const date = new Date(t.date);
    const normalizedDate = isNaN(date.getTime()) ? t.date : date.toISOString().split('T')[0];

    return `${normalizedDate}|${normalizedAmount}|${normalizedDesc}`;
  };

  console.log(`üîç Deduplication: Processing ${transactions.length} transactions...`);

  transactions.forEach((transaction, index) => {
    const key = createTransactionKey(transaction);

    if (transactionKeys.has(key)) {
      // This is a duplicate
      const originalTransaction = transactionKeys.get(key);
      duplicateTransactions.push({
        ...transaction,
        isDuplicate: true,
        duplicateOf: key,
      });
      console.log(`  ‚ö†Ô∏è  Duplicate found: ${transaction.date} - ${transaction.description} - $${transaction.amount}`);
    } else {
      // This is unique
      transactionKeys.set(key, transaction);
      uniqueTransactions.push({
        ...transaction,
        isDuplicate: false,
      });
    }
  });

  console.log(`  ‚úì Unique: ${uniqueTransactions.length}, Duplicates: ${duplicateTransactions.length}`);

  return { uniqueTransactions, duplicateTransactions };
}

/**
 * Analyze bank statements using OpenAI GPT-4
 */
export async function analyzeStatements(
  files: Express.Multer.File[],
  currentHousingPayment: number
): Promise<CashFlowAnalysis> {
  try {
    console.log(`Processing ${files.length} files...`);
    console.log(`‚ö° Using parallel processing for ${files.length} files simultaneously`);

    // Process ALL files in PARALLEL for maximum performance
    const filePromises = files.map(async (file) => {
      console.log(`üìÑ Starting: ${file.originalname}`);
      try {
        const content = await processFile(file);
        console.log(`‚úì Completed: ${file.originalname}`);
        return { success: true, content, filename: file.originalname };
      } catch (error) {
        console.error(`‚úó Failed: ${file.originalname}:`, error instanceof Error ? error.message : error);
        return { success: false, error, filename: file.originalname };
      }
    });

    // Wait for all files to process (or fail) in parallel
    const fileResults = await Promise.allSettled(filePromises);

    // Extract successful results and failed files
    const extractedData: string[] = [];
    const failedFiles: string[] = [];

    fileResults.forEach((result) => {
      if (result.status === 'fulfilled') {
        const fileResult = result.value;
        if (fileResult.success && 'content' in fileResult) {
          // @ts-ignore - content is checked above
          extractedData.push(fileResult.content);
        } else if (!fileResult.success) {
          failedFiles.push(fileResult.filename);
        }
      } else {
        failedFiles.push('unknown file');
      }
    });

    console.log(`‚ö° Parallel processing complete: ${extractedData.length} successful, ${failedFiles.length} failed`);

    if (extractedData.length === 0) {
      throw new Error(`Failed to process any files. ${failedFiles.length} file(s) failed: ${failedFiles.join(', ')}`);
    }

    if (failedFiles.length > 0) {
      console.log(`‚ö†Ô∏è  Warning: ${failedFiles.length} file(s) failed to process: ${failedFiles.join(', ')}`);
      console.log(`‚úì Successfully processed ${extractedData.length} file(s), continuing with analysis...`);
    }

    const combinedData = extractedData.join('\n\n---NEW DOCUMENT---\n\n');

    console.log('üîç Analyzing combined transaction data with AI...');
    console.log(`üìä Combined data length: ${combinedData.length} characters (~${estimateTokens(combinedData)} tokens)`);

    // DECISION POINT: Use chunking for large datasets
    // Try to parse as JSON to count transactions
    let parsedTransactions: any[] = [];
    let shouldUseChunking = false;

    try {
      // Attempt to parse as JSON array
      parsedTransactions = JSON.parse(combinedData);

      if (Array.isArray(parsedTransactions)) {
        console.log(`üìä Detected ${parsedTransactions.length} transactions`);

        // Use chunking if > 500 transactions (to ensure each chunk fits in 16K token output)
        if (parsedTransactions.length > 500) {
          console.log(`‚ö° Using CHUNKED processing (${parsedTransactions.length} > 500 transactions)`);
          shouldUseChunking = true;
        } else {
          console.log(`‚úì Using SINGLE-REQUEST processing (${parsedTransactions.length} <= 500 transactions)`);
        }
      }
    } catch (parseError) {
      console.log('üìù Data is not a JSON array, using single-request processing');
    }

    // CHUNKED PROCESSING for large datasets
    if (shouldUseChunking && parsedTransactions.length > 0) {
      const chunks = chunkTransactions(parsedTransactions, 250);

      console.log(`\nüöÄ Processing ${chunks.length} chunks in parallel...`);
      const startTime = Date.now();

      // Process all chunks in parallel
      const chunkPromises = chunks.map((chunk, index) =>
        analyzeTransactionChunk(chunk, index + 1, chunks.length, currentHousingPayment)
      );

      const chunkResults = await Promise.all(chunkPromises);
      const elapsedTime = Date.now() - startTime;

      console.log(`\n‚ö° All chunks processed in ${(elapsedTime / 1000).toFixed(1)}s`);

      // Merge chunk results
      const result = mergeChunkResults(chunkResults);

      // Continue with deduplication and final processing...
      const { uniqueTransactions, duplicateTransactions } = deduplicateTransactions(result.transactions || []);
      const flaggedTransactions = uniqueTransactions.filter((t: any) => t.flagged === true);
      const averageMonthlyBalance = Math.max(0, result.netCashFlow);

      console.log('üìà Final Analysis Results:');
      console.log(`  ‚úì Total transactions: ${result.transactions?.length || 0}`);
      console.log(`  ‚úì Unique transactions: ${uniqueTransactions.length}`);
      console.log(`  ‚úì Duplicate transactions: ${duplicateTransactions.length}`);
      console.log(`  ‚úì Flagged for review: ${flaggedTransactions.length}`);
      console.log(`  ‚úì Net cash flow: $${result.netCashFlow || 0}`);

      // Clean up files
      for (const file of files) {
        try {
          if (file.path) {
            await fs.unlink(file.path);
          }
        } catch (error) {
          console.error(`Error deleting file ${file.path}:`, error);
        }
      }

      return {
        totalIncome: result.totalIncome || 0,
        totalExpenses: result.totalExpenses || 0,
        netCashFlow: result.netCashFlow || 0,
        averageMonthlyBalance,
        transactions: uniqueTransactions,
        monthlyBreakdown: result.monthlyBreakdown || [],
        flaggedTransactions,
        duplicateTransactions,
        confidence: result.confidence || 0.7,
      };
    }

    // SINGLE-REQUEST PROCESSING for smaller datasets (original code path)
    const dataToAnalyze = combinedData;
    const estimatedTokens = estimateTokens(dataToAnalyze);

    console.log(`‚úì Processing all data in single request (~${estimatedTokens} tokens)`);

    if (estimatedTokens > 100000) {
      console.log(`‚ö†Ô∏è  Warning: Large dataset (${estimatedTokens} tokens). This may take longer to process.`);
    }

    // Use GPT-4 to analyze the transactions with enhanced anomaly detection
    const prompt = `You are a financial analysis expert. You are receiving PRE-PARSED STRUCTURED DATA from bank statements.

‚ö†Ô∏è CRITICAL: The data below is ALREADY EXTRACTED as JSON. Each transaction is already parsed with date, description, and amount.
‚ö†Ô∏è YOUR JOB: Categorize and analyze these existing transactions. DO NOT extract or re-interpret them.
‚ö†Ô∏è PRESERVE ALL TRANSACTIONS: You must include EVERY transaction in your output. Do not drop any transactions.

IMPORTANT: You must be CONSISTENT and DETERMINISTIC. Analyze the same data the same way every time.

Current housing payment (to exclude from expenses): $${currentHousingPayment}

Pre-Parsed Transaction Data (JSON Array):
${dataToAnalyze}

Your task is to CATEGORIZE and ANALYZE the above transactions:

1. **PRESERVE & CATEGORIZE EVERY TRANSACTION**:
   - The data is already extracted - keep ALL transactions in your output
   - Use the EXACT date, description, and amount from the input
   - DO NOT drop transactions, modify dates, or change amounts
   - DO NOT hallucinate new transactions or dates

   CATEGORIZATION RULES:
   - "income": Deposits, paychecks, Zelle/transfers IN (positive amounts or credits)
   - "expense": Regular recurring expenses like utilities, bills, subscriptions, groceries
   - "housing": Rent or mortgage payments (around $${currentHousingPayment})
   - "one-time": Irregular purchases, large transfers, one-time payments

2. **FLAG ANOMALIES FOR REVIEW**:
   Flag transactions that are:
   - **Luxury items**: High-end dining, designer purchases, jewelry, luxury travel
   - **Unusually large**: 2x or more above typical spending in that category
   - **One-time purchases**: Major purchases, large cash withdrawals, wire transfers
   - **Irregular deposits**: Large one-time deposits, refunds, bonuses
   - **Non-essential**: Entertainment, hobbies, discretionary spending above normal patterns

   Provide specific flag reasons like:
   - "One-time: Wire transfer"
   - "Unusually large payment"
   - "Luxury: High-end purchase"
   - "Irregular: Large deposit"

3. **MONTHLY BREAKDOWN**:
   Group by month (YYYY-MM format) and calculate:
   - Total income per month (sum of income category)
   - Total expenses per month (sum of expense category, EXCLUDING housing and flagged items)
   - Net cash flow per month
   - Transaction count per month

4. **CALCULATE TOTALS**:
   - SUM of ALL income transactions across entire period
   - SUM of ALL expense transactions (EXCLUDING housing and flagged one-time items)
   - Net cash flow (total income - total expenses)

5. **CONFIDENCE SCORE**: Your confidence in categorization accuracy (0-1 scale)

CRITICAL RULES - READ CAREFULLY:
‚úì PRESERVE EVERY TRANSACTION - Your output must have the SAME number of transactions as the input
‚úì USE EXACT DATES - Do not change or hallucinate dates (use YYYY-MM-DD format from input)
‚úì USE EXACT AMOUNTS - Do not modify transaction amounts
‚úì USE EXACT DESCRIPTIONS - Keep original descriptions from input
‚úì DETERMINISTIC - Same input MUST produce same output every time
‚úì NO SAMPLING - Include 100% of transactions, not a sample
‚úì CATEGORIZE ONLY - You are categorizing existing data, not extracting new data

Return COMPACT JSON (minimize whitespace, omit empty flagReason for unflagged items):
{
  "transactions": [
    {"date": "YYYY-MM-DD", "description": "Description", "amount": 1234.56, "category": "income", "flagged": false, "monthYear": "2024-08"},
    {"date": "YYYY-MM-DD", "description": "Description", "amount": -125.50, "category": "expense", "flagged": true, "flagReason": "One-time: Large purchase", "monthYear": "2024-08"}
  ],
  "monthlyBreakdown": [
    {"month": "2024-08", "income": 5000.00, "expenses": 2500.00, "netCashFlow": 2500.00, "transactionCount": 45}
  ],
  "totalIncome": 5000.00,
  "totalExpenses": 2500.00,
  "netCashFlow": 2500.00,
  "confidence": 0.85
}

‚ö†Ô∏è IMPORTANT: Use compact formatting. Omit "flagReason" key entirely when flagged=false to save tokens.`;

    // Create a timeout promise for the main analysis call
    const ANALYSIS_TIMEOUT_MS = 120000; // 120 seconds
    const analysisTimeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => {
        console.error(`‚è±Ô∏è  Timeout reached for main analysis (${ANALYSIS_TIMEOUT_MS / 1000}s)`);
        reject(new Error(`Transaction analysis timed out after ${ANALYSIS_TIMEOUT_MS / 1000} seconds`));
      }, ANALYSIS_TIMEOUT_MS);
    });

    // Use OpenAI Direct (GPT-4o) for final text analysis
    // This analyzes the extracted transaction text (from CSV or vision extraction)
    console.log(`üß† Using OpenAI Direct (${TEXT_MODEL}) for transaction analysis...`);
    console.log(`üîí Deterministic mode: temperature=0, top_p=1, seed=42 for consistent results`);
    const analysisApiCallPromise = openaiDirect.chat.completions.create({
      model: TEXT_MODEL,
      messages: [
        {
          role: 'system',
          content: 'You are a financial analyst specialized in cash flow analysis. You must be consistent and deterministic. Always respond with valid JSON. Extract the same transactions from the same data every time.',
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      response_format: { type: 'json_object' },
      temperature: 0, // Fully deterministic for consistent JSON formatting
      top_p: 1, // Disable nucleus sampling for maximum determinism
      seed: 42, // Fixed seed for reproducible results across identical inputs
      max_tokens: 16384, // GPT-4o's maximum output token limit
    }, {
      timeout: ANALYSIS_TIMEOUT_MS,
    });

    console.log(`üèÅ Racing main analysis against ${ANALYSIS_TIMEOUT_MS / 1000}s timeout...`);
    // Race the API call against the timeout
    const response = await Promise.race([analysisApiCallPromise, analysisTimeoutPromise]);
    console.log('‚úÖ AI analysis completed successfully');

    // Log system fingerprint for reproducibility verification
    if (response.system_fingerprint) {
      console.log(`üîë System fingerprint: ${response.system_fingerprint} (use this to verify consistent backend)`);
    }

    // Parse JSON with better error handling
    const rawContent = response.choices[0]?.message?.content || '{}';
    console.log(`üìù Raw response length: ${rawContent.length} chars`);

    let result;
    try {
      result = JSON.parse(rawContent);
    } catch (parseError) {
      console.error('‚ùå JSON Parse Error:', parseError);
      console.log('üîç Attempting to extract JSON from response...');

      // Try to find JSON in markdown code blocks
      const jsonMatch = rawContent.match(/```(?:json)?\s*(\{[\s\S]*\})\s*```/);
      if (jsonMatch) {
        console.log('‚úì Found JSON in markdown code block, retrying parse...');
        result = JSON.parse(jsonMatch[1]);
      } else {
        // Log first and last 500 chars for debugging
        console.error('First 500 chars:', rawContent.substring(0, 500));
        console.error('Last 500 chars:', rawContent.substring(rawContent.length - 500));
        throw parseError;
      }
    }

    // Clean up uploaded files after processing
    for (const file of files) {
      try {
        await fs.unlink(file.path);
      } catch (error) {
        console.error(`Error deleting file ${file.path}:`, error);
      }
    }

    // Calculate average monthly balance (net cash flow that can offset principal)
    const averageMonthlyBalance = Math.max(0, result.netCashFlow);

    // Extract flagged transactions for review
    const flaggedTransactions = (result.transactions || []).filter((t: any) => t.flagged === true);

    // Deduplicate transactions across files
    const { uniqueTransactions, duplicateTransactions } = deduplicateTransactions(result.transactions || []);

    console.log('üìà Analysis Results:');
    console.log(`  ‚úì Total transactions: ${result.transactions?.length || 0}`);
    console.log(`  ‚úì Unique transactions: ${uniqueTransactions.length}`);
    console.log(`  ‚úì Duplicate transactions: ${duplicateTransactions.length}`);
    console.log(`  ‚úì Flagged for review: ${flaggedTransactions.length}`);
    console.log(`  ‚úì Monthly deposits: $${result.totalIncome || 0}`);
    console.log(`  ‚úì Monthly expenses: $${result.totalExpenses || 0}`);
    console.log(`  ‚úì Net cash flow: $${result.netCashFlow || 0}`);
    console.log(`  ‚úì Confidence: ${((result.confidence || 0) * 100).toFixed(0)}%`);

    return {
      totalIncome: result.totalIncome || 0,           // Sum of all income (frontend divides by months)
      totalExpenses: result.totalExpenses || 0,       // Sum of all expenses (frontend divides by months)
      netCashFlow: result.netCashFlow || 0,           // Total net (frontend divides by months)
      averageMonthlyBalance,
      transactions: uniqueTransactions,
      monthlyBreakdown: result.monthlyBreakdown || [],
      flaggedTransactions,
      duplicateTransactions,
      confidence: result.confidence || 0.7,
    };
  } catch (error) {
    console.error('Error in analyzeStatements:', error);
    throw new Error(`Failed to analyze bank statements: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * NEW ARCHITECTURE: Extract transactions from files (Step 1 of 2)
 * Just does extraction, returns raw transaction data without categorization
 */
export async function extractTransactions(
  files: Express.Multer.File[]
): Promise<any[]> {
  try {
    console.log(`üìÑ Extracting transactions from ${files.length} file(s)...`);

    // Process ALL files in PARALLEL for maximum performance
    const filePromises = files.map(async (file) => {
      console.log(`üìÑ Starting: ${file.originalname}`);
      try {
        const content = await processFile(file);
        console.log(`‚úì Completed: ${file.originalname}`);
        return { success: true, content, filename: file.originalname };
      } catch (error) {
        console.error(`‚úó Failed: ${file.originalname}:`, error instanceof Error ? error.message : error);
        return { success: false, error, filename: file.originalname };
      }
    });

    // Wait for all files to process (or fail) in parallel
    const fileResults = await Promise.allSettled(filePromises);

    // Extract successful results and failed files
    const extractedData: string[] = [];
    const failedFiles: string[] = [];

    fileResults.forEach((result) => {
      if (result.status === 'fulfilled') {
        const fileResult = result.value;
        if (fileResult.success && 'content' in fileResult) {
          // @ts-ignore - content is checked above
          extractedData.push(fileResult.content);
        } else if (!fileResult.success) {
          failedFiles.push(fileResult.filename);
        }
      } else {
        failedFiles.push('unknown file');
      }
    });

    console.log(`‚ö° Extraction complete: ${extractedData.length} successful, ${failedFiles.length} failed`);

    if (extractedData.length === 0) {
      throw new Error(`Failed to extract data from any files. ${failedFiles.length} file(s) failed: ${failedFiles.join(', ')}`);
    }

    if (failedFiles.length > 0) {
      console.log(`‚ö†Ô∏è  Warning: ${failedFiles.length} file(s) failed to process: ${failedFiles.join(', ')}`);
      console.log(`‚úì Successfully extracted from ${extractedData.length} file(s), continuing...`);
    }

    // Combine all extracted data
    const combinedData = extractedData.join('\n\n---NEW DOCUMENT---\n\n');

    // Parse into transaction array
    let transactions: any[] = [];
    try {
      // Try to parse as JSON array
      transactions = JSON.parse(combinedData);
      if (!Array.isArray(transactions)) {
        // If it's not an array, try to parse each line as a transaction
        const lines = combinedData.split('\n').filter(line => line.trim() && line.includes('|'));
        transactions = lines.map(line => {
          const parts = line.split('|').map(p => p.trim());
          if (parts.length >= 3) {
            return {
              date: parts[0],
              description: parts[1],
              amount: parseFloat(parts[2].replace(/[^0-9.-]/g, ''))
            };
          }
          return null;
        }).filter(t => t !== null);
      }
    } catch (parseError) {
      // If parsing fails, try line-by-line parsing
      const lines = combinedData.split('\n').filter(line => line.trim() && line.includes('|'));
      transactions = lines.map(line => {
        const parts = line.split('|').map(p => p.trim());
        if (parts.length >= 3) {
          return {
            date: parts[0],
            description: parts[1],
            amount: parseFloat(parts[2].replace(/[^0-9.-]/g, ''))
          };
        }
        return null;
      }).filter(t => t !== null);
    }

    console.log(`‚úÖ Extracted ${transactions.length} raw transactions`);

    // Clean up files
    for (const file of files) {
      try {
        await fs.unlink(file.path);
      } catch (error) {
        console.error(`Error deleting file ${file.path}:`, error);
      }
    }

    return transactions;
  } catch (error) {
    console.error('Error in extractTransactions:', error);
    throw new Error(`Failed to extract transactions: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * NEW ARCHITECTURE: Categorize a chunk of transactions (Step 2 of 2)
 * Takes raw transaction data and categorizes it using AI
 */
export async function categorizeTransactions(
  transactions: any[],
  currentHousingPayment: number,
  chunkNumber: number,
  totalChunks: number
): Promise<OpenAIAnalysisResult> {
  try {
    console.log(`\nüîÑ Categorizing chunk ${chunkNumber}/${totalChunks} (${transactions.length} transactions)...`);

    const chunkDataStr = JSON.stringify(transactions);
    console.log(`   üìä Chunk size: ${chunkDataStr.length} characters (~${estimateTokens(chunkDataStr)} tokens)`);

    const prompt = `You are a financial analysis expert analyzing a CHUNK of transactions from bank statements.

‚ö†Ô∏è CRITICAL: This is CHUNK ${chunkNumber} of ${totalChunks}. Analyze ONLY these transactions independently.
‚ö†Ô∏è The data is ALREADY EXTRACTED as JSON. Your job: Categorize and analyze these transactions.
‚ö†Ô∏è PRESERVE ALL TRANSACTIONS: Include EVERY transaction in your output.

Current housing payment (to exclude from expenses): $${currentHousingPayment}

Transaction Data for Chunk ${chunkNumber}:
${chunkDataStr}

Your task: CATEGORIZE and ANALYZE these transactions:

CATEGORIZATION RULES:
- "income": Deposits, paychecks, Zelle/transfers IN (positive amounts or credits)
- "expense": Regular recurring expenses like utilities, bills, subscriptions, groceries
- "housing": Rent or mortgage payments (around $${currentHousingPayment})
- "one-time": Irregular purchases, large transfers, one-time payments

FLAG ANOMALIES:
- Luxury items, unusually large purchases, one-time events, irregular deposits
- Provide specific flag reasons when flagged=true

CRITICAL RULES:
‚úì PRESERVE EVERY TRANSACTION - Output count must match input count (${transactions.length} transactions)
‚úì USE EXACT DATES/AMOUNTS/DESCRIPTIONS from input
‚úì DETERMINISTIC - Same input = same output
‚úì NO SAMPLING - Include 100% of transactions

Return COMPACT JSON (omit flagReason when flagged=false):
{
  "transactions": [
    {"date": "YYYY-MM-DD", "description": "Description", "amount": 1234.56, "category": "income", "flagged": false, "monthYear": "2024-08"}
  ],
  "totalIncome": 5000.00,
  "totalExpenses": 2500.00,
  "confidence": 0.85
}`;

    const CHUNK_TIMEOUT_MS = 30000; // 30 seconds per chunk (fast categorization)

    const response = await openaiDirect.chat.completions.create({
      model: TEXT_MODEL,
      messages: [
        {
          role: 'system',
          content: 'You are a financial analyst specialized in cash flow analysis. Always respond with valid JSON. Be consistent and deterministic.',
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      response_format: { type: 'json_object' },
      temperature: 0,
      top_p: 1,
      seed: 42,
      max_tokens: 16384,
    }, {
      timeout: CHUNK_TIMEOUT_MS,
    });

    const rawContent = response.choices[0]?.message?.content || '{}';
    const result = JSON.parse(rawContent);

    console.log(`‚úÖ Chunk ${chunkNumber}/${totalChunks} completed:`);
    console.log(`   üìä Transactions returned: ${result.transactions?.length || 0}/${transactions.length}`);
    console.log(`   üí∞ Income: $${result.totalIncome?.toFixed(2) || 0}, Expenses: $${result.totalExpenses?.toFixed(2) || 0}`);

    if (result.transactions?.length !== transactions.length) {
      console.warn(`‚ö†Ô∏è  Chunk ${chunkNumber} transaction count mismatch! Expected ${transactions.length}, got ${result.transactions?.length || 0}`);
    }

    return result;
  } catch (error) {
    console.error(`‚ùå Chunk ${chunkNumber}/${totalChunks} failed:`, error);
    throw error;
  }
}
